{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTwpDNBnorUT"
      },
      "source": [
        "# ColBERTv2: Indexing & Search Notebook\n",
        "\n",
        "If you're working in Google Colab, we recommend selecting \"GPU\" as your hardware accelerator in the runtime settings.\n",
        "\n",
        "First, we'll import the relevant classes. Note that `Indexer` and `Searcher` are the key actors here. Next, we'll download the necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_YBBPTo5AZ",
        "outputId": "863fe0d5-9457-4660-d44a-6c875b85d9dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: cannot change to 'ColBERT/': No such file or directory\n",
            "Cloning into 'ColBERT'...\n",
            "remote: Enumerating objects: 2835, done.\u001b[K\n",
            "remote: Counting objects: 100% (1169/1169), done.\u001b[K\n",
            "remote: Compressing objects: 100% (353/353), done.\u001b[K\n",
            "remote: Total 2835 (delta 948), reused 817 (delta 816), pack-reused 1666 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2835/2835), 2.07 MiB | 4.56 MiB/s, done.\n",
            "Resolving deltas: 100% (1785/1785), done.\n"
          ]
        }
      ],
      "source": [
        "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
        "import sys; sys.path.insert(0, 'ColBERT/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmBi2UT5pxb3",
        "outputId": "407ea417-2491-4831-e51b-1b1cdb1d0ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.3.1\n",
            "Obtaining file:///content/ColBERT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitarray (from colbert-ai==0.2.20)\n",
            "  Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Collecting datasets (from colbert-ai==0.2.20)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (3.1.0)\n",
            "Collecting git-python (from colbert-ai==0.2.20)\n",
            "  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\n",
            "Collecting python-dotenv (from colbert-ai==0.2.20)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ninja (from colbert-ai==0.2.20)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.20) (4.47.1)\n",
            "Collecting ujson (from colbert-ai==0.2.20)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting faiss-gpu>=1.7.0 (from colbert-ai==0.2.20)\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting torch==1.13.1 (from colbert-ai==0.2.20)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->colbert-ai==0.2.20) (4.12.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->colbert-ai==0.2.20)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->colbert-ai==0.2.20)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->colbert-ai==0.2.20)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->colbert-ai==0.2.20)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->colbert-ai==0.2.20) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->colbert-ai==0.2.20) (0.45.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->colbert-ai==0.2.20)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (2.32.3)\n",
            "Collecting xxhash (from datasets->colbert-ai==0.2.20)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->colbert-ai==0.2.20)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->colbert-ai==0.2.20)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.20) (6.0.2)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.20) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.20) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.20) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.20) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.20) (1.9.0)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from git-python->colbert-ai==0.2.20) (3.1.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.20) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.20) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->colbert-ai==0.2.20) (0.4.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->colbert-ai==0.2.20) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask->colbert-ai==0.2.20) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->colbert-ai==0.2.20) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->colbert-ai==0.2.20) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->colbert-ai==0.2.20) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->colbert-ai==0.2.20) (2024.12.14)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->git-python->colbert-ai==0.2.20) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->colbert-ai==0.2.20) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->colbert-ai==0.2.20) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->colbert-ai==0.2.20) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.20) (5.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->colbert-ai==0.2.20) (1.17.0)\n",
            "Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m180.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "Downloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Installing collected packages: faiss-gpu, bitarray, xxhash, ujson, python-dotenv, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, ninja, fsspec, dill, nvidia-cudnn-cu11, multiprocess, torch, git-python, datasets, colbert-ai\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[33m  DEPRECATION: Legacy editable install of colbert-ai[faiss-gpu,torch]==0.2.20 from file:///content/ColBERT (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py develop for colbert-ai\n"
          ]
        }
      ],
      "source": [
        "try: # When on google Colab, let's install all dependencies with pip.\n",
        "    import google.colab\n",
        "    !pip install -U pip\n",
        "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
        "except Exception:\n",
        "  import sys; sys.path.insert(0, 'ColBERT/')\n",
        "  try:\n",
        "    from colbert import Indexer, Searcher\n",
        "  except Exception:\n",
        "    print(\"If you're running outside Colab, please make sure you install ColBERT in conda following the instructions in our README. You can also install (as above) with pip but it may install slower or less stable faiss or torch dependencies. Conda is recommended.\")\n",
        "    assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0jxbVar4kln"
      },
      "outputs": [],
      "source": [
        "import colbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQg9A-dtp1nB"
      },
      "outputs": [],
      "source": [
        "from colbert import Indexer, Searcher\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert.data import Queries, Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLWjmlkVq9r0"
      },
      "source": [
        "We will use the dev set of the **LoTTE benchmark** we recently introduced in the ColBERTv2 paper. We'll download it from HuggingFace datasets. The dev and test sets contain several domain-specific corpora, and we'll use the smallest dev set corpus, namely lifestyle:dev.\n",
        "\n",
        "For the purposes of a quick demo, we will only run the `Indexer` on the first 10,000 passages. As we do this, let's also remove the queries whose relevant passages are all outside this small set of passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF7lv8jvq-ut"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = 'lifestyle'\n",
        "datasplit = 'dev'\n",
        "\n",
        "collection_dataset = load_dataset(\"colbertv2/lotte_passages\", dataset)\n",
        "collection = [x['text'] for x in collection_dataset[datasplit + '_collection']]\n",
        "\n",
        "queries_dataset = load_dataset(\"colbertv2/lotte\", dataset)\n",
        "queries = [x['query'] for x in queries_dataset['search_' + datasplit]]\n",
        "\n",
        "f'Loaded {len(queries)} queries and {len(collection):,} passages'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXKC1oeUsnhk"
      },
      "source": [
        "This loaded 417 queries and 269k passages. Let's inspect one query and one passage to verify we have done so correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQFUHYTZs0aa",
        "outputId": "22f80aad-b6de-4df4-ffde-dd4b7718d78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "are blossom end rot tomatoes edible?\n",
            "\n",
            "I think the spraying thing is not after, it's during. The cold will freeze the mist, keeping the air around the trees at (but not below) freezing. See http://www.ehow.com/how_5805520_use-freeze-damage-fruit-trees.html for example which recommends a sprinkler. The \"releases heat\" thing is kind of an oversimplification, but basically as long as you have any liquid water around, it will keep things at zero. The sap of your tree is not pure water, and therefore freezes somewhat below zero. By having the water freeze instead you stay away from the temps that would damage your plants. That said, http://www.ehow.com/how-does_5245655_spraying-frost-protect-fruit-freezing_.html is total gibberish since evaporation doesn't generate heat, quite the opposite. There is a better explanation at http://www.gardenguides.com/135830-spray-water-plants-during-frost.html This is a picture from a blog entry that gives you details from the citrus farmer's point of view.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(queries[24])\n",
        "print()\n",
        "print(collection[19929])\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for id , text in enumerate(collection):\n",
        "  print({id}, {text})"
      ],
      "metadata": {
        "id": "LO1diQr5DF3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJdAAbDu7PZ"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "For an efficient search, we can pre-compute the ColBERT representation of each passage and index them.\n",
        "\n",
        "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKAdVN5MvDKD"
      },
      "outputs": [],
      "source": [
        "nbits = 2   # encode each dimension with 2 bits\n",
        "doc_maxlen = 300 # truncate passages at 300 tokens\n",
        "max_id = 10000\n",
        "\n",
        "index_name = f'{dataset}.{datasplit}.{nbits}bits'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsImrM-rAUzi"
      },
      "source": [
        "To save space and time, we will only run the `Indexer` on the first 10,000 passages. To do so, we will filter out queries that do not contain passages with ids less than 10,000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kDkbZCvY_f7x",
        "outputId": "16b2c417-4fdd-447c-8fa6-0a67486d7c94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Filtered down to 20 queries'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "answer_pids = [x['answers']['answer_pids'] for x in queries_dataset['search_' + datasplit]]\n",
        "filtered_queries = [q for q, apids in zip(queries, answer_pids) if any(x < max_id for x in apids)]\n",
        "\n",
        "f'Filtered down to {len(filtered_queries)} queries'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orKfQRmQv46u"
      },
      "source": [
        "Now run the `Indexer` on the collection subset. Assuming the use of only one GPU, this cell should take about six minutes to finish running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRiOnzxtwI0j",
        "outputId": "ddcd7c74-81fc-428d-b34a-3cdeb868c58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[Jun 28, 22:27:33] #> Creating directory /content/experiments/notebook/indexes/lifestyle.dev.2bits \n",
            "\n",
            "\n",
            "#> Starting...\n",
            "#> Joined...\n"
          ]
        }
      ],
      "source": [
        "checkpoint = 'colbert-ir/colbertv2.0'\n",
        "\n",
        "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
        "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
        "                                                                                # Consider larger numbers for small datasets.\n",
        "\n",
        "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
        "    indexer.index(name=index_name, collection=collection[:max_id], overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CTbP2LS1xHVZ",
        "outputId": "e20fed9f-14e8-4028-ec3b-34bba0a960c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/experiments/notebook/indexes/lifestyle.dev.2bits'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "indexer.get_index() # You can get the absolute path of the index, if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY6_D523yBFB"
      },
      "source": [
        "## Search\n",
        "\n",
        "Having built the index and prepared our `searcher`, we can search for individual query strings.\n",
        "\n",
        "We can use the `queries` set we loaded earlier — or you can supply your own questions. Feel free to get creative! But keep in mind this set of ~300k lifestyle passages can only answer a small, focused set of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3x_FnVnyB0n",
        "outputId": "739b356b-cc76-4b30-8e1e-a150d8719cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Jun 28, 22:34:54] #> Loading codec...\n",
            "[Jun 28, 22:34:54] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jun 28, 22:34:54] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jun 28, 22:34:55] #> Loading IVF...\n",
            "[Jun 28, 22:34:55] #> Loading doclens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1261.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Jun 28, 22:34:55] #> Loading codes and residuals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 10.74it/s]\n"
          ]
        }
      ],
      "source": [
        "# To create the searcher using its relative name (i.e., not a full path), set\n",
        "# experiment=value_used_for_indexing in the RunConfig.\n",
        "with Run().context(RunConfig(experiment='notebook')):\n",
        "    searcher = Searcher(index=index_name, collection=collection)\n",
        "\n",
        "\n",
        "# If you want to customize the search latency--quality tradeoff, you can also supply a\n",
        "# config=ColBERTConfig(ncells=.., centroid_score_threshold=.., ndocs=..) argument.\n",
        "# The default settings with k <= 10 (1, 0.5, 256) gives the fastest search,\n",
        "# but you can gain more extensive search by setting larger values of k or\n",
        "# manually specifying more conservative ColBERTConfig settings (e.g. (4, 0.4, 4096))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JYA0N22yIeS",
        "outputId": "7dda63ff-24e3-407b-fd2d-db74b8b27f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#> are some cats just skinny?\n",
            "\n",
            "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
            "#> Input: . are some cats just skinny?, \t\t True, \t\t None\n",
            "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2024,  2070,  8870,  2074, 15629,  1029,   102,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103])\n",
            "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "\t [1] \t\t 24.1 \t\t A cat can certainly be naturally skinny. I know one who was the runt of her litter and has been extremely thin all her life, to the point where you can easily count every bone. She is now 17 years old, having outlived two other cats in that household, so it certainly doesn't seem to have held her back.\n",
            "\t [2] \t\t 24.0 \t\t Yes. Just like us, cats vary in size and shape and weight. And like us, some of that is diet, some is health, some is genetics, some is age. One of my lady's cats was the runt of the litter; it wasn't certain she would survive, and she has always been both small and skinny. It hasn't seemed to limit her climbing/jumping much, if at all; I think she benefits from square/cube law to be stronger relative to her weight than you would expect. Another cat in the family was not only longer/taller/broader but also more solidly muscled. I think he may have weighed twice what the small one did, without being overweight. (The simplest rule-of-thumb test for whether a cat is overweight: if you can count every vertibra as you run your hand down their back, they're fine. If some or all of them are hard to feel through the skin, there's probably more fat there than there should be. The runt really is below ideal weight; not only can you feel every bone in her back, it's somewhat hard to believe there are muscles and tendons there. Especially true now that she's a senior catizen.)\n",
            "\t [3] \t\t 22.1 \t\t Thank you for your question. You have a few different issues within your one question, but let's start with your main question regarding cats eating each other's food. I have the same problem with my cats. I have one cat who gorges herself, and I have one skinny cat who likes to graze all day long. What do you do with this situation? It is not easy. You have to get a little creative. And unfortunately you will likely never get to the point where they will only eat their own food. However, you can get to a point where they mostly eat their own food. One thing to note is that cats tend to be dominant over certain things. Cats are not usually dominant over everything, like dogs tend to be. Some cats are dominant over food, which it sounds like your cat is. My thin cat will not eat unless my overweight cat is eating. You can see the obvious dominance there. Here are some ideas for you... KEEPING CATS FROM EATING EACH OTHER'S FOOD I feed my cats wet food at night. I feed the slightly overweight cat in one place and my thin cat on a high shelf where she can see the other cat eating. During the day, I feed my overweight cat using an interactive feeder, so she has to use her natural instincts to work for her food. I give my skinny cat a small bowl of dry food on another shelf, so she can graze. It's not that my overweight cat can't get to the other food, but she is content to eat her easier-to-get food. Each morning, I also feed my skinny cat dry food in our closet, which is at the end of our walk-in bathroom. So there is quite a buffer from the bathroom door. I turn on the fan, so the other cat (who is closed out) cannot hear my skinny cat crunching. The things we do for our cats! I also sneak my skinny cat treats now and then. CATS EATING DOG FOOD This is a real problem for inside cats. You have to stop this. Even though your cats may be healthy now, eating dog food will make any cat overweight and unhealthy. I have volunteered at an animal shelter for many years and have seen this problem over and over again with overweight cats given to the shelter. Feed your dog at times you can supervise and take up the bowl when your dog is finished. If you kennel your dog, feed him in his kennel, so the cats can't get to the food. CAN CATS EAT VEGAN FOOD This is the most difficult part of your question because I can tell you have strong convictions in this area. However, I must tell you that cats should not ever be on a vegan diet. I am very sorry. I know this is difficult to hear. Cats were built to be obligate carnivores. They must have the taurine from the meat because they cannot produce it on their own, like dogs can. I know some vegan pet foods have synthetic taurine, but I wouldn't chance this. It is just not the same. Cats on a vegan diet will likely have severe medical conditions. See the quote below from Pet WebMD If allowed to continue long enough, these dietary problems can lead to serious and sometimes irreversible medical conditions. The one veterinarians mention most often is taurine-related dilated cardiomyopathy (enlarged heart with weak contractions and poor pumping ability). Low taurine can also lead to reproductive failures, growth failures, and eye problems. I hope this helps and at least gives you some ideas. I wish you and your animal family the best!\n"
          ]
        }
      ],
      "source": [
        "query = filtered_queries[13] # try with an in-range query or supply your own\n",
        "print(f\"#> {query}\")\n",
        "\n",
        "# Find the top-3 passages for this query\n",
        "results = searcher.search(query, k=3)\n",
        "\n",
        "# Print out the top-k retrieved passages\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}